---
layout: post
title: Strassen Algorithm and GEMM
date: 2023-12-03 10:51:07 -0700
category: CUDA
author: üçì and üç≥
---

Overview
--------

In this mini project, we implemented the Strassen algorithm, a recursive
matrix-matrix multiplication algorithm that reduces the complexity from
\\(\mathcal{O}(n^3)\\) to
\\(\mathcal{O}(n^{\log_2 8}) \approx \mathcal{O}(n^{2.8})\\), and custom GEMM
kernels with a few optimizations on both CPU (Ryzen 9 5950x) and GPU (NVIDIA RTX
3090). Both speed and accuracy of these programs are profiled and compared.

### two versions of Strassen

There are two versions of the Strassen algorithm that have been implemented in
our project. The first version is the basic Strassen, where 17 memory
allocations are required at each level of the recursion for storing intermediate
results of sub-matrices [[1]](https://en.wikipedia.org/wiki/Strassen_algorithm).
The second version is a modified version where the result matrix \\(C\\) is used
to store most of the intermediate results, and only two temporary matrices are
required for additional storage at each recursion level
[[2]](https://www.cise.ufl.edu/~sahni/papers/strassen.pdf).

### how Strassen operations are handled on GPU

For matrix additions at each level and the terminal matrix multiplication, we
used the `cuBLAS` library provided by CUDA to perform the operations. The reason
is that `cuBLAS` is a highly optimized library for all the BLAS operations on
NVIDIA GPUs, so we think it can provide us with the best performance achievable
with running Strassen on GPU and would make the comparison later more
meaningful.

Recursion is handled on the CPU, which is also where the `cuBLAS` kernels are
called at each recursion level. We didn‚Äôt do it on GPU because `cuBLAS` isn‚Äôt
invokable on GPU, and recursion has a lot of limitations if called on GPU, such
as stack size. We can probably do some more experiment with GPU-based recursion
(a.k.a. device-side kernel launch) later on, where we use our own kernels and
try to get around the above limitations.

### GEMM on CPU

We implemented a single-threaded GEMM function with blocktiling on CPU. Two
temporary matrices of tunable sizes are allocated to store intermediate blocks
of \\(A\\) and \\(B\\) during the computation, and the purpose of these blocks
is to help with the cache behavior on the CPU. Blocks of matrix \\(B\\) are
loaded in the outer loops to fill the L3 cache in a typical CPU, whereas
(smaller) blocks of matrix \\(A\\) are loaded in the inner loops to fill the L2
cache.

Even more sub-divided tiles of matrices \\(A\\) and \\(B\\) are then extracted
to accumulate results in a tile of the matrix \\(C\\), and both tiles would be
leveraging the L1 cache. The computation of above mentioned tiles is often
referred to as a "microkernel", and is often a major focus of CPU GEMM
optimization, where engineers leverage vectorized data, loop unrolling, and
architecture-specific assembly code to improve its performance as much as
possible. As our focus in this project is mostly on GPU, we have simplified our
microkernel in CPU GEMM to a very basic 3 for-loop implementation.

### GEMM on GPU

The basic kernel setup is:

- Divide the matrix \\(C\\) to be computed into a 2D grid comprised of 2D
  blocks, with tunable block size in each dimension
- Each thread block computes one block of the result matrix \\(C\\) that
  corresponds to a block row in matrix \\(A\\) and a block column in matrix
  \\(B\\)
- Each thread computes a tile in matrix \\(C\\), which corresponds to a tile in
  \\(A\\) and a tile in \\(B\\)

We employed a few optimizations on the GPU. The first one is blocking, which
takes advantage of the shared memory on the GPU which is faster than global
memory. This is a common optimization used in GEMM kernels on GPU. Blocks from
row in matrix \\(A\\) and column in \\(B\\) are loaded into shared memory and
results are accumulated before the next block is loaded.

The second optimization is loading blocks of matrix \\(A\\) while transposing
each of them. This serves as a preparation for the next step, where elements in
tiles of \\(A\\) are accessed column by column to compute the tile in \\(C\\).
Therefore, loading in transposed way can transform the access into row by row,
which is a better accessing pattern for cache (in this case, shared memory).

The third optimization is tiling, where we load a row or column within the tile
from the block of matrix \\(A\\) and \\(B\\) from shared memory into local
arrays of the threads and compute the corresponding tile in matrix \\(C\\). This
takes advantage of the large amount of registers on GPU as local array elements
are stored in registers local to each thread. This is much faster than accessing
the global memory every time, and each load would fully reuse all the elements
in the registers before replacing them with the next tile row and column.

And the final optimization is using vectorized data types to load and store the
elements, and in our case, it‚Äôs the `float4`. We divide the number of `float4`
to load and store among each thread. The threads will access these `float4`
contiguously by row.

Some Insights
-------------

### input matrix

We generated pairs of \\(2^n \times 2^n\\) (where \\(n = 10, 12, 14\\)) matrices
as input, where each element is a floating-point number randomly generated
between 0 and 1. The terminal matrix size, which determines where to stop the
recursion, is selected as \\(2^{n-2}, 2^{n-6}, 2^{n-8}\\) for each of the above
\\(n\\). This results in 2, 6, 8 levels of recursion being analyzed.

|  | Strassen v1 | Strassen v1 CPU | Strassen v2 | Strassen v2 CPU | BT | BT CPU | cuBLAS |
| --- | --- | --- | --- | --- | --- | --- | --- |
| `10/8` | 0.085 | 1.261 | 0.069 | 0.867 | 0.059 | 0.379 | 0.062 |
| `10/4` | 1.174 | 0.249 | 1.228 | 0.261 |  |  |  |
| `10/2` | 51.178 | 0.333 | 56.922 | 1.241 |  |  |  |
| `12/10` | 0.115 | 228.687 | 0.125 | 207.391 | 0.091 | 25.174 | 0.117 |
| `12/6` | 1.628 | 28.540 | 1.680 | 17.258 |  |  |  |
| `12/4` | 55.419 | 12.337 | 58.027 | 12.914 |  |  |  |
| `14/12` | 0.902 | 47753.0 | 0.930 | 45947.3 | 1.007 | 1604.74 | 0.903 |
| `14/8` | 2.712 | 7589.05 | 7.437 | 2517.41 |  |  |  |
| `14/6` | 74.616 | 2228.13 | 83.051 | 775.900 |  |  |  |

<span class="img-caption">Table 1: execution time in seconds, BT = blocktiling</span>

### recursion handling on CPU and GPU

From the results obtained above, we found that not only does CPU handle
recursion well, it can in fact benefit from the reduced complexity introduced by
the Strassen algorithm. The deeper the recursion goes, the less time it requires
the CPU to compute the Strassen algorithm. When the matrix is larger, such as
\\(2^{12} \times 2^{12}\\), the deeper recursions can achieve even faster than
the blocktiling implementation, about \\(2\times\\) faster. This shouldn‚Äôt be
taken too rigorously though, as we didn‚Äôt employ multithreading on CPU with
either method.

The observations are almost the opposite on GPU. With both versions of Strassen,
as the recursion goes deeper, the performance drops significantly, especially
when the matrix size goes up, where more than \\(70\times\\) to \\(80\times\\)
slowdown compared to two levels of recursion is measured. Not sure how the
result would be if only device kernels are used.

### recursion and GEMM

We know from the results that, in general, GEMM performs much better than the
Strassen algorithm, especially on GPU, but with two exceptions:

- With recursion of only two levels, on GPU the Strassen algorithm performs a
  bit better than the blocktiling
- When using the version with 17 memory allocations on CPU, the Strassen
  algorithm with deeper recursion out performs the blocktiling

### memory allocation impact

On GPU, if we consider the most common cases, implementation of the Strassen
algorithm with two temporary allocations at each level is always a bit faster
than the one with 17 memory allocations. However, when we dive into CPU
performance, the version with 17 allocations out performs the two allocation
version, and it‚Äôs even more significant when the matrix size is larger such as
\\(2^{14} \times 2^{14}\\) and the recursion level isn‚Äôt too small. This may be
due to the fact that memory and cache is limited on the GPU and frequent
allocation and deallocation is very expensive. But CPU has multiple levels of
cache and its size is larger, so the version with 17 allocations can better take
advantage of this resource.

We also tried allocating the temporary matrices before launching the kernels in
the modified version with two allocations, and because the operations are
executed sequentially for each level, they can reuse the same memory. But this
brought little performance benefit, we guess it may have something to do with
the compiler optimization, but we cannot be certain.

### error analysis

We used the `cuBLAS` result of the same matrix multiplication as our baseline.
Strassen algorithm in either version has higher numerical error compared to
blocktiling. The approximate ratio is that, at \\(2^{10} \times 2^{10}\\), as
recursion goes deeper, the error of Strassen algorithm to blocktiling goes from
\\(1\times\\) to \\(200\times\\), at \\(2^{12} \times 2^{12}\\) and
\\(2^{14} \times 2^{14}\\), our blocktiling implementation has the exact same
result as cuBLAS (which internally uses the CUTLASS kernel) so the error is 0.0,
and for Strassen algorithm, one more level of recursion would incur one
magnitude (\\(10\times\\)) of higher error. In conclusion, the larger the matrix
and deeper the recursion both leads to higher error in Strassen algorithm. We
further speculate that this result and the higher error in Strassen algorithm
compared to blocktiling is due to the significant amount of matrix additions
involved in the process.

Next Steps
----------

Some other ideas that we may explore next when we are not lazy:

- **Write a kernel to invoke Strassen from device**: Leverage device-side kernel
  launch find out how to resolve the limitations and how it impacts performance
- **Improve the CPU version with multithreading and microkernel optimizations**:
  Explore whether combining the reduced complexity and optimized matrix matrix
  operations can help CPU achieve performance closer to GPU when matrix is of
  certain sizes
- **Double buffering with the GEMM CUDA kernel**: This is way to achieve some
  form of instruction level parallelism on GPU -- not sure how it will affect
  the occupancy though since two blocks of shared memory are used for each block
  in the result, but we are intrigued to find out
- **Error analysis with analytic-form matrices**: Using input and output
  matrices with specific structure where the exact output can be calculated can
  be benefitial for analyzing floating-point errors and allow us to compare the
  arithmetic results with more solid ground