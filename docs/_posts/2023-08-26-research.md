---
layout: post
title: "Research Overview"
date: 2023-08-26 01:13:18 -0700
---

My primary research topic is on efficient and scalable Gaussian Process (GP)
regression. We leverage sparse matrices and apply various linear algebra
routines for function evaluations to improve the time complexity and robustness
of GP models. Engineering wise, we parallelize our implementation on both the
CPU and GPU with data parallelism and tools such as CUDA to achieve higher
performance.

Technically speaking, GP is a probability distribution over possible functions
given certain training data. It can be viewed as a supervised machine learning
model for making predictions with some prior knowledge of the data. In
addition, it can also provide uncertainty measures, known as the confidence
region, for any predictions it is responsible for.

The loss function when optimizing a GP model is defined as:

\\[
    \log \mathcal{P}(\mathbf{y}|\mathbf{X}, \mathbf{\theta}) =
    - \dfrac{1}{2}\mathbf{y}^{\top} K_y^{-1}\mathbf{y}
    - \dfrac{1}{2}\log |K_y|
    - \dfrac{n}{2}\log 2\pi
\\]

where \\(K_y\\) is the covariance matrix, \\(\mathbf{X}\\) is the observed
points, \\(\theta\\) is the model parameters, and \\(\mathbf{y}\\) is the
function outputs of \\(\mathbf{X}\\) with noise. The most computationally
expensive parts are the inverse \\(K_y^{-1}\\) and the logarithm of determinant
\\(\log |K_y|\\), where common methods such as
[Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition)
or [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) would take
\\(\mathcal{O}(n^3)\\) time.

To reduce this complexity, we are studying the use of
[conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)
and [Lanczos algorithm](https://en.wikipedia.org/wiki/Lanczos_algorithm) in the
calculation of \\(K_y^{-1}\\) and \\(\log |K_y|\\). The reason why these
iterative methods could work is that usually after some iterations
\\(m \ll n\\), we can have a very good approximation \\(\hat{K_y}\\) of the
original covariance matrix, but the dimension is greatly reduced. We are also
exploring sparse matrices to take advantage of the large amount of zero entries
in the covariance matrix. Finally, by leveraging the high computing power of GPU
for such types of problems, we can further improve the performance of GP on
certain specialized types of HPCs with GPU support.

On the other hand, to improve robustness, we propose to use numerical algorithms
to replace the traditional approach of automatic differentiation, where all the
gradients are computed based on differentiation formulae. As a result, the
GP convergence tends to become smoother in our particular training process.

Implementation wise, currently we are using [`PyTorch`](https://pytorch.org/)
and its `torch.distributions` API for building our GP model. We use the `Adam`
optimizer since it has the desirable adaptive learning rate which would be
helpful during training. The computation of \\(K_y^{-1}\\) is currently
implemented with conjugate gradient, and for \\(\log |K_y|\\), we are currently
studying the use of trace estimation.

To get a sense of the impact of our current work, below is a comparison of the
convergence contour plots with our implementation compared to the highly
optimized one in [`GPyTorch`](https://gpytorch.ai/) -- another topic I covered
more extensively in
[this blog](https://thenewstack.io/using-gpytorch-a-researchers-experience/)
sponsored by the Linux Foundation:

![](/assets/gp_compare.png)
<span class="img-caption">
    Left: our implementation, right: GPyTorch's implementation
</span>

In both figures, the deeper the color of the contour, the more optimal the loss
value is. Both training starts from the pink dot on the right side, and
progresses towards the orange dot as the destination after 80 iterations. It is
quite obvious that our implementation converges more stably towards the optima,
while `GPyTorch`'s implementation jumps back and forth a bit in the final few
iterations. This would demonstrate that our method exhibits a more robust
convergence pattern.
