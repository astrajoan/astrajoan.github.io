---
layout: post
title: Using cudaMalloc (cudaFree) in Ctor (Dtor)
date: 2023-10-05 10:25:51 -0700
category: CUDA
author: üçì
---

The main reasons to put `cudaMalloc` and `cudaFree` in class constructor and
destructor is that it uses RAII and offers automatic resource management and
exception safety. I want it to behave like STL containers such as `std::vector`
where the resources it owns are automatically destroyed when it goes out of
scope. Although there‚Äôs no clear ownership in the device memory case, since all
we can do on host (our class) is to hold a pointer to the resource residing in
device memory, it would still be good to structure the code in a way that, the
class which would own the resource in main memory if everything was implemented
on CPU would also be in control of the same resource on GPU, which means
managing its allocation and deallocation. This also brings exception safety,
where if an exception is thrown after the object is constructed, the destructor
will still be called, ensuring that resources are freed.

Issue
-----

However, as good as it sounds, after some searching, I found out that this
design has some issues when it comes to global or static objects. The primary
concern is the uncertainty of the destruction order when `exit()` is called.
This can lead to the case where the CUDA context might be destroyed before these
objects are destructed, especially when `exit()` is called or when dealing with
static duration objects because of the complications arising due to C++'s
non-guaranteed order of destruction for static objects. This can lead to
undefined behavior or errors.

Another issue comes with instantiating an object outside of `main`. At that
time, the CUDA context is not setup yet, but the object relies on it, so there
would be an error when trying to instantiate it.

Workarounds
-----------

### explicit resource management

The simplest solution would be to explicitly manage CUDA resources using
dedicated initialization and cleanup functions. This makes sure to create
objects after the CUDA context is initialized and destroy them before the
context is terminated. This however, doesn‚Äôt really go with my idea to have
some form of automatic resource management.

### using smart pointers

This approach uses smart pointers with custom deleters. It lets us have
automatic resource management while ensuring that `cudaFree` is called at the
appropriate time.

### control object lifetime

In this method, we need to ensure that objects managing CUDA resources have a
shorter lifetime than the CUDA context. It can be achieved in several ways:

- Instantiate CUDA resource-managing objects within a local scope that is
  guaranteed to end before the CUDA context is destroyed
- Or, design a lifecycle management class that initializes the CUDA context at
  creation and cleans it up at destruction; create objects managing CUDA
  resources only after this class is instantiated and ensure they are destroyed
  before this class's destructor is called

Maybe also avoid using global or static objects for managing CUDA resources
since their destruction time compare to CUDA context cannot be guaranteed.
However, sometimes it is not fully up to us to choose which objects should
control which resources.

Conclusion
----------

To be exact, I didn‚Äôt have this issue with my implementation, because my objects
are all local and non-static. Their construction is within `main` and
destruction is guaranteed to happen before `main` returns (before their scope
ends), and as CUDA context is destroyed only after `main` return, these objects
can safely call `cudaMalloc` in their constructors and `cudaFree` in their
destructors. But it‚Äôs good to know that if we ever need a static object which
should own resources in device memory, its resource management should be
performed explicitly or should directly control the CUDA context management, not
simply relying on the constructors and destructors.

References
----------

- [cuda call fails in destructor](https://stackoverflow.com/q/35815597/22266244)
- [How to handle cudaFree on globally instantiated variables](https://stackoverflow.com/questions/43235548/how-to-handle-cudafree-on-globally-instantiated-variables)
- [Calling CUDA Commands in Destructors](https://github.com/FLAMEGPU/FLAMEGPU2/issues/190#issue-568403926)
